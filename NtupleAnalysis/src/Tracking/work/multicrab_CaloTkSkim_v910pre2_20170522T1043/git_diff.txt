diff --git a/NtupleAnalysis/python/tools/datasets.py b/NtupleAnalysis/python/tools/datasets.py
index 284a9b45861..4f656ebdad0 100644
--- a/NtupleAnalysis/python/tools/datasets.py
+++ b/NtupleAnalysis/python/tools/datasets.py
@@ -1,4 +1,3 @@
-#lumiMask = "/afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions16/13TeV/Cert_271036-276811_13TeV_PromptReco_Collisions16_JSON.txt" # ICHEP dataset 271036-276811
 lumiMask = "/afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions16/13TeV/Cert_271036-284044_13TeV_PromptReco_Collisions16_JSON_NoL1T.txt"
 
 
@@ -13,7 +12,6 @@ class Dataset:
         self.DBS = dbs
         self.dataVersion = dataVersion
         if not os.path.dirname(lumiMask):
-            #lumiMask = os.path.join(os.environ['CMSSW_BASE'],"src/HiggsAnalysis/MiniAOD2TTree/data",lumiMask)
             lumiMask = os.path.join("", "src/HiggsAnalysis/MiniAOD2TTree/data",lumiMask)
         self.lumiMask = lumiMask
         self.DASquery = dasQuery
@@ -118,18 +116,20 @@ TP2015Datasets.extend(datasetsMinBias_UpgFall13d)
 TP2015Datasets.extend(datasetsTTbar_UpgFall13d)
 TP2015Datasets.extend(datasetsTauThreeProngs_UpgFall13d)
 
+ID2017Datasets = []
+ID2017Datasets.extend(datasetsBs_PhaseIISpring17D)
+ID2017Datasets.extend(datasetsSingleE_PhaseIISpring17D)
+ID2017Datasets.extend(datasetsSingleMu_PhaseIISpring17D)
+ID2017Datasets.extend(datasetsSingleNu_PhaseIISpring17D)
+ID2017Datasets.extend(datasetsSinglePhoton_PhaseIISpring17D)
+ID2017Datasets.extend(datasetsSinglePion0_PhaseIISpring17D)
+ID2017Datasets.extend(datasetsSinglePion_PhaseIISpring17D)
+ID2017Datasets.extend(datasetsSingleTau_PhaseIISpring17D)
+ID2017Datasets.extend(datasetsTTJpsiFilter_PhaseIISpring17D)
+ID2017Datasets.extend(datasetsTT_PhaseIISpring17D)
+ID2017Datasets.extend(datasetsTTbar_PhaseIISpring17D)
+
 TDR2019Datasets = []
-TDR2019Datasets.extend(datasetsBs_PhaseIISpring17D)
-TDR2019Datasets.extend(datasetsSingleE_PhaseIISpring17D)
-TDR2019Datasets.extend(datasetsSingleMu_PhaseIISpring17D)
-TDR2019Datasets.extend(datasetsSingleNu_PhaseIISpring17D)
-TDR2019Datasets.extend(datasetsSinglePhoton_PhaseIISpring17D)
-TDR2019Datasets.extend(datasetsSinglePion0_PhaseIISpring17D)
-TDR2019Datasets.extend(datasetsSinglePion_PhaseIISpring17D)
-TDR2019Datasets.extend(datasetsSingleTau_PhaseIISpring17D)
-TDR2019Datasets.extend(datasetsTTJpsiFilter_PhaseIISpring17D)
-TDR2019Datasets.extend(datasetsTT_PhaseIISpring17D)
-TDR2019Datasets.extend(datasetsTTbar_PhaseIISpring17D)
 
 
 #================================================================================================ 
@@ -175,12 +175,14 @@ class DatasetGroup:
         Create dataset grouping in a dictionary for easy access.
         '''
 
-        analyses = ["SignalAnalysis", "Hplus2tbAnalysis", "TauLeg", "METLeg", "L1Study", "All"]
+        analyses = ["TP2015", "ID2017", "TDR2019", "All"]
+
         if self.analysis not in analyses:
             raise Exception("Unknown analysis \"%s\". Please select one of the following: \"%s" % (self.analysis, "\", \"".join(analyses) + "\".") )
 
 
         self.GroupDict["TP2015"]  = TP2015Datasets
+        self.GroupDict["ID2017"]  = ID2017Datasets
         self.GroupDict["TDR2019"] = TDR2019Datasets
         self.GroupDict["All"]     = TP2015Datasets + TDR2019Datasets
         return
diff --git a/NtupleAnalysis/scripts/hltausPseudoMulticrab.py b/NtupleAnalysis/scripts/hltausPseudoMulticrab.py
index 0e4196b4aed..f3c44012f2d 100755
--- a/NtupleAnalysis/scripts/hltausPseudoMulticrab.py
+++ b/NtupleAnalysis/scripts/hltausPseudoMulticrab.py
@@ -1,8 +1,8 @@
 #!/usr/bin/env python
 '''
 Description:
-This script is used to create a pseudo multicrab directory, using an input ROOT file (miniaod2tree.root).
-The purpose is primarily to enable the easy testing of local changes to the MiniAOD2TTree code and the 
+This script is used to create a pseudo multicrab directory, using an input ROOT file (raw2TTree.root).
+The purpose is primarily to enable the easy testing of local changes to the  code and the 
 NtupleAnalysis code using the hplusGenerateDataFormats.py script.
 A script execution will thus create an empty multicrab with identical name and structure as those created
 by the multicrab.py script. It will contain a single dataset with a single ROOT file under results/ dir, which is 
@@ -234,7 +234,7 @@ def moveAllHistosIntoAnalysisFolder(fileName, opts):
             k.Delete()
 
     # Close the ROOT file
-    Print("Closing file %s." % (fOUT.GetName()) )
+    Verbose("Closing file %s." % (fOUT.GetName()) )
     fOUT.Close()
     return
 
@@ -349,7 +349,7 @@ def GetAnalysis():
     Verbose("GetAnalysis()")
     
     # Create a compiled regular expression object
-    leg_re = re.compile("miniAOD2TTree_(?P<leg>\S+)_cfg.py")
+    leg_re = re.compile("raw2TTree_(?P<leg>\S+)_cfg.py")
 
     # Scan through the string 'pwd' & look for any location where the compiled RE 'cmssw_re' matches
     match = leg_re.search(opts.pset)
@@ -370,7 +370,7 @@ def AskToContinue(taskDirName, analysis, opts):
     '''
     Verbose("AskToContinue()")
 
-    Print("Creating pseudo-multicrab directory \"%s\" using the file \"%s\" as input" % (taskDirName, opts.rootFile) )
+    Print("Creating pseudo-multicrab directory \"%s\" using the file \"%s\" as input." % (taskDirName, opts.rootFile) )
     #DatasetGroup(analysis).PrintDatasets(False)
     
     AbortTask(keystroke="q")
@@ -382,10 +382,9 @@ def AbortTask(keystroke):
     Give user last chance to abort CRAB task creation.
     '''
     Verbose("AbortTask()")
-    if not opts.ask:
-        return
 
-    message  = "=== %s:\n\tPress \"%s\" to abort, any other key to proceed: " % (GetSelfName(), keystroke)
+    #message  = "=== %s:\n\tPress \"%s\" to abort, any other key to proceed: " % (GetSelfName(), keystroke)
+    message  = "\tPress \"%s\" to abort, any other key to proceed: " % (keystroke)
     response = raw_input(message)
     if (response!= keystroke):
         return
@@ -467,45 +466,31 @@ def GetRequestName(dataset):
     '''
     Verbose("GetRequestName()")
     
-    # Create compiled regular expression objects
-    datadataset_re = re.compile("^/(?P<name>\S+?)/(?P<run>Run\S+?)/")
+    # New regular expressions for HLTausAnalysis
     mcdataset_re   = re.compile("^/(?P<name>\S+?)/")
-    tune_re        = re.compile("(?P<name>\S+)_Tune")
-    tev_re         = re.compile("(?P<name>\S+)_13TeV")
-    ext_re         = re.compile("(?P<name>_ext\d+)-")
-    runRange_re    = re.compile("Cert_(?P<RunRange>\d+-\d+)_")
-    # runRange_re    = re.compile("Cert_(?P<RunRange>\d+-\d+)_13TeV_PromptReco_Collisions15(?P<BunchSpacing>\S*)_JSON(?P<Silver>(_\S+|))\.")
-    # runRange_re    = re.compile("Cert_(?P<RunRange>\d+-\d+)_13TeV_PromptReco_Collisions15(?P<BunchSpacing>\S*)_JSON")
-    # runRange_re    = re.compile("Cert_(?P<RunRange>\d+-\d+)_13TeV_PromptReco_Collisions15_(?P<BunchSpacing>\d+ns)_JSON_v")
-    
-    # Scan through the string 'dataset.URL' & look for any location where the compiled RE 'mcdataset_re' matches
+    tune_re        = re.compile("Tune\w+_")
+    tev_re         = re.compile("\d*TeV")
+    pileup_re      = re.compile("\w*PU\d*")
+
+    # Scan through the string 'dataset' & look for any location where the compiled RE 'mcdataset_re' matches
     match = mcdataset_re.search(dataset.URL)
-    if dataset.isData():
-	match = datadataset_re.search(dataset.URL)
-        
-    # Append the dataset name
     if match:
-	requestName = match.group("name")
-
-    # Append the Run number (for Data samples only)
-    if dataset.isData():
-	requestName+= "_"
-	requestName+= match.group("run")
+        requestName = match.group().split("_")[0]        
 
-    # Append the MC-tune (for MC samples only) 
-    tune_match = tune_re.search(requestName)
+    # Append the MC-tune
+    tune_match = tune_re.search(dataset.URL)
     if tune_match:
-	requestName = tune_match.group("name")
+        requestName += "_" + tune_match.group()
 
-    # Append the COM Energy (for MC samples only) 
-    tev_match = tev_re.search(requestName)
+    # Append the COM Energy
+    tev_match = tev_re.search(dataset.URL)
     if tev_match:
-	requestName = tev_match.group("name")
+        requestName += tev_match.group()
 
-    # Append the Ext
-    ext_match = ext_re.search(dataset.URL)
-    if ext_match:
-	requestName+=ext_match.group("name")
+    # Append the Pileup
+    pileup_match = pileup_re.search(dataset.URL)
+    if pileup_match:
+        requestName += "_" + pileup_match.group()
 
     # Append the Run Range (for Data samples only)
     if dataset.isData():
@@ -521,7 +506,7 @@ def GetRequestName(dataset):
 
     # Finally, replace dashes with underscores    
     requestName = requestName.replace("-","_")
-
+    requestName = requestName.replace("/","")
     return requestName
 
 
@@ -551,12 +536,11 @@ def CreateJob(opts, args):
     Verbose("CreateJob()")
     
     # Get general info
-    version     = GetCMSSW(opts)
-    analysis    = GetAnalysis()
-    dataset = None
-    for d in DatasetGroup("All").GetDatasetList():
-        if opts.dataset in d.URL.replace("-", "_"):
-            dataset = d
+    version  = GetCMSSW(opts)
+    analysis = GetAnalysis()
+    dataset  = None
+    for d in DatasetGroup(opts.dataEra).GetDatasetList():
+        dataset = d
     if dataset == None:
         raise Exception("Could not find dataset object for dataset with name \"%s\"." % (opts.dataset) )
     else:
@@ -576,34 +560,29 @@ def CreateJob(opts, args):
 
     # Create the "multicrab.cfg" file
     multicrabCfg = open(taskDirName + "/" + "multicrab.cfg", 'a')
-        
-    # For-loop: All datasets [always 1 in this case]
-    for dataset in datasets:        
-	Verbose("Determining request name for dataset with URL \"%s\"" % (dataset.URL))
-        requestName = GetRequestName(dataset)
-
-	Verbose("Creating directory for dataset with request name \"%s\"" % (requestName))
-        datasetDir = os.path.join(taskDirName, requestName)
-        if os.path.exists(datasetDir) and os.path.isdir(datasetDir):
-            raise Exception("Cannot create directory \"%s\". It already exists!" % (datasetDir))
-        else:
-            os.mkdir(datasetDir)
 
-        # Write dataset to multicrab.cfg
-        multicrabCfg.write("[" + str(requestName) + "]")
-        multicrabCfg.write("\n")
+    # Create the dataset subdirectory
+    datasetDir   = os.path.join(taskDirName, opts.dataset)
+    if os.path.exists(datasetDir) and os.path.isdir(datasetDir):
+        raise Exception("Cannot create directory \"%s\". It already exists!" % (datasetDir))
+    else:
+        os.mkdir(datasetDir)
+        
+    # Write the new dataset to multicrab.cfg
+    multicrabCfg.write("[" + str(opts.dataset) + "]")
+    multicrabCfg.write("\n")
         
-	Verbose("Creating directory structure for dataset with request name \"%s\"" % (requestName))
-        dirs = ["results", "inputs"]
-        for d in dirs:
-            newDir = os.path.join(datasetDir, d)
-            if os.path.exists(newDir) and os.path.isdir(newDir):
-                raise Exception("Cannot create directory \"%s\". It already exists!" % (newDir))
-            else:
-                os.mkdir(newDir)
+    Verbose("Creating directory structure for dataset with name \"%s\"" % (opts.dataset))
+    dirs = ["results", "inputs"]
+    for d in dirs:
+        newDir = os.path.join(datasetDir, d)
+        if os.path.exists(newDir) and os.path.isdir(newDir):
+            raise Exception("Cannot create directory \"%s\". It already exists!" % (newDir))
+        else:
+            os.mkdir(newDir)
 
         resultsDir  = os.path.join(datasetDir, "results")
-        resultsFile = "histograms-%s.root" % (requestName)
+        resultsFile = "histograms-%s.root" % (opts.dataset)
         resultsPath = os.path.join(resultsDir, resultsFile)
 	Verbose("Copying the ROOT file \"%s\" in the directory \"%s\"" % (opts.rootFile, resultsDir))
         cmd = "cp %s %s" % (opts.rootFile, resultsPath)
@@ -613,7 +592,7 @@ def CreateJob(opts, args):
         moveAllHistosIntoAnalysisFolder(resultsPath, opts)
         writeCounters(resultsPath, opts)
 
-    Print("Successfully created pseudo-multicrab directory \"%s\" " % (taskDirName))
+    Print("Successfully created pseudo-multicrab directory \"%s\" " % (taskDirName), False)
     multicrabCfg.close()
     if 0:
         os.system("ls -lt")
@@ -640,12 +619,12 @@ if __name__ == "__main__":
     # Default Values
     VERBOSE      = False
     ASK          = False
-    PSET         = "miniAOD2TTree_CaloTk_cfg.py"
+    PSET         = "raw2TTree_CaloTkSkim_cfg.py"
     DIRNAME      = ""
-    DATASET      = ""
-    CMSSW        = "61XSLHC6"
+    DATASET      = None #"VBF_HToTauTau_125_14TeV_powheg_pythia6"
+    CMSSW        = "910pre2" #"61XSLHC6"
     ANALYSIS     = "HLTausAnalysis"
-    DATAERA      = "TP2015" #"TDR2019"
+    DATAERA      = "ID2017"
     DATAVERSION  = CMSSW + "mc"
     ROOTFILE     = "test.root"
     NOMINALLUMI  = 5 # 5e34 (<PU>=140)
@@ -656,11 +635,11 @@ if __name__ == "__main__":
     parser.add_option("-v", "--verbose", dest="verbose", default=VERBOSE, action="store_true",
                       help="Verbose mode for debugging purposes [default: %s]" % (VERBOSE))
 
-    parser.add_option("--dataset", dest="dataset", default="VBF_HToTauTau_125_14TeV_powheg_pythia6", 
+    parser.add_option("--dataset", dest="dataset", default=DATASET, 
                       help="Dataset to include in multicrab dir [default: %s]" % (DATASET))
 
     parser.add_option("-f", "--rootFile", dest="rootFile", default= ROOTFILE,
-                      help="The ROOT file (miniaod2tree.root) to be copied inside the multicrab dir[default: %s]" % (ROOTFILE) )
+                      help="The ROOT file (raw2TTree.root) to be copied inside the multicrab dir[default: %s]" % (ROOTFILE) )
 
     parser.add_option("-p", "--pset", dest="pset", default=PSET, type="string",
                       help="The python cfg file to be used by cmsRun [default: %s]" % (PSET))
@@ -693,11 +672,10 @@ if __name__ == "__main__":
     if len(sys.argv) < 1:
         parser.print_help()
         sys.exit(1)
-    else:
-        pass
+        
     # The ROOT file options must be provided
     if opts.rootFile == None:
-        raise Exception("Must provide a ROOT file (miniaod2tree.root) as argument!")
+        raise Exception("Must provide a ROOT file (raw2TTree.root) as argument!")
 
     # Luminosity setting
     validLumis = [NOMINALLUMI, ULTIMATELUMI]
@@ -705,6 +683,12 @@ if __name__ == "__main__":
         Print("Invalid value for lumi (\"%s\"). Luminosity value must correspond to either the HL-LHC nominal luminosity %s (E+34) or the HL-LHC ultimate luminosity %s (E+34)." % (opts.lumi, NOMINALLUMI, ULTIMATELUMI), True)
         sys.exit()
 
+    # Data-era setting
+    validEras = ["TP2015", "ID2017", "TDR2019"]
+    if opts.dataEra not in validEras:
+        Print("Invalid data-eta \"%s\". Please select one of the following:\n\t%s" % (opts.dataEra, ", ".join(validEras)), True)
+        sys.exit()
+
     # Pileup setting        
     if opts.lumi == NOMINALLUMI:
         opts.pileup = 140
@@ -713,26 +697,21 @@ if __name__ == "__main__":
     Print("The average pileup for the HL-LHC luminosity %sE+34 is set to <PU>=%s" % (opts.lumi, opts.pileup), True)
 
     # Dataset check
-    validDataset  = True
-    validDatasets = []
-    for d in DatasetGroup("All").GetDatasetList():
-        dName = GetRequestName(d)
-        validDatasets.append(dName)
-        if opts.dataset == dName:
-            validDataset = True
-        else:
-            pass
-
-    if not validDataset:
-        Print("Invalid dataset %s. Please select one of the following:\n\t" % (opts.dataset, ", ".join(validDatasets)), True)
+    if opts.dataset is None:
+        text = 'sample'
+        try:
+            opts.dataset = re.search('histograms-(.+?).root', opts.rootFile).group(1)
+            Print("File \"%s\" corresponds to dataset is \"%s\""% (opts.rootFile, opts.dataset), False)
+        except AttributeError:
+            raise Exception("Could not determine the dataset corresponding to the file \"%s\"" % (opts.rootFile) )
+
+    # For-loop: All data-era datasets
+    datasets     = DatasetGroup(opts.dataEra).GetDatasetList()
+    datasetNames = [GetRequestName(d) for d in datasets]
+    if opts.dataset not in datasetNames:
+        Print("Invalid dataset \"%s\". Please select one of the following:\n\t%s" % (opts.dataset, "\n\t".join(datasetNames)), True)
         sys.exit()
     
-    # Additional sanity checks
-    datasets = ["VBF_HToTauTau_125_14TeV_powheg_pythia6", "Neutrino_Pt2to20_gun", "PYTHIA6_Tauola_TTbar_TuneZ2star_14TeV", "TauThreeProngs"]
-    if opts.dataset not in datasets:
-        Print("Dataset %s not valid. Please select one of the following:\n\t%s" % (opts.dataset, ", ".join(datasets)))
-        sys.exit()
-
     # Create the pseudo-multicrab dir (if the ROOT file exists)
     if os.path.exists(opts.rootFile):
         sys.exit( CreateJob(opts, args) )
diff --git a/NtupleAnalysis/src/Auxiliary/src/Datasets.C b/NtupleAnalysis/src/Auxiliary/src/Datasets.C
index 46579c41a3a..ac8fcd63c7d 100644
--- a/NtupleAnalysis/src/Auxiliary/src/Datasets.C
+++ b/NtupleAnalysis/src/Auxiliary/src/Datasets.C
@@ -258,25 +258,25 @@ void Datasets::CreateMcProductions_(void)
   cmssw    = "9_0_0";
   geometry = "Phase2";
   
-  Datasets SingleNeutrinoPU140_PhaseIISpring17D("SingleNeutrino-PU140", "/SingleNeutrino" + path_PU140, "", CP, cmssw, geometry, 140, 500000, 0, 0);
-  Datasets SingleNeutrinoPU200_PhaseIISpring17D("SingleNeutrino-PU200", "/SingleNeutrino" + path_PU200, "", CP, cmssw, geometry, 140, 500000, 0, 0);
+  Datasets SingleNeutrinoPU140_PhaseIISpring17D("SingleNeutrino_PU140", "/SingleNeutrino" + path_PU140, "", CP, cmssw, geometry, 140, 500000, 0, 0);
+  Datasets SingleNeutrinoPU200_PhaseIISpring17D("SingleNeutrino_PU200", "/SingleNeutrino" + path_PU200, "", CP, cmssw, geometry, 140, 500000, 0, 0);
   
-  Datasets SinglePion0NoPU_PhaseIISpring17D("SinglePion0-NoPU"  , "/SinglePion0_FlatPt-8to100" + path_NoPU , "", CP, cmssw, geometry,   0, 500000, 0, 0);
-  Datasets SinglePion0PU140_PhaseIISpring17D("SinglePion0-PU140", "/SinglePion0_FlatPt-8to100" + path_PU140, "", CP, cmssw, geometry, 140, 500000, 0, 0);
-  Datasets SinglePion0PU200_PhaseIISpring17D("SinglePion0-PU200", "/SinglePion0_FlatPt-8to100" + path_PU200, "", CP, cmssw, geometry, 200, 499400, 0, 0);
+  Datasets SinglePion0NoPU_PhaseIISpring17D("SinglePion0_NoPU"  , "/SinglePion0_FlatPt-8to100" + path_NoPU , "", CP, cmssw, geometry,   0, 500000, 0, 0);
+  Datasets SinglePion0PU140_PhaseIISpring17D("SinglePion0_PU140", "/SinglePion0_FlatPt-8to100" + path_PU140, "", CP, cmssw, geometry, 140, 500000, 0, 0);
+  Datasets SinglePion0PU200_PhaseIISpring17D("SinglePion0_PU200", "/SinglePion0_FlatPt-8to100" + path_PU200, "", CP, cmssw, geometry, 200, 499400, 0, 0);
   
-  Datasets SinglePionNoPU_PhaseIISpring17D("SinglePion-NoPU"  , "/SinglePion_FlatPt-8to100" + path_NoPU , "", CP, cmssw, geometry,   0, 492588, 0, 0);
-  Datasets SinglePionPU140_PhaseIISpring17D("SinglePion-PU140", "/SinglePion_FlatPt-8to100" + path_PU140, "", CP, cmssw, geometry, 140, 499850, 0, 0);
-  Datasets SinglePionPU200_PhaseIISpring17D("SinglePion-PU200", "/SinglePion_FlatPt-8to100" + path_PU200, "", CP, cmssw, geometry, 200, 498400, 0, 0);
+  Datasets SinglePionNoPU_PhaseIISpring17D("SinglePion_NoPU"  , "/SinglePion_FlatPt-8to100" + path_NoPU , "", CP, cmssw, geometry,   0, 492588, 0, 0);
+  Datasets SinglePionPU140_PhaseIISpring17D("SinglePion_PU140", "/SinglePion_FlatPt-8to100" + path_PU140, "", CP, cmssw, geometry, 140, 499850, 0, 0);
+  Datasets SinglePionPU200_PhaseIISpring17D("SinglePion_PU200", "/SinglePion_FlatPt-8to100" + path_PU200, "", CP, cmssw, geometry, 200, 498400, 0, 0);
   
-  Datasets SingleTauNoPU_PhaseIISpring17D("SingleTau-NoPU"  , "SingleTau_NoPU" , "/SingleTau_FlatPt-8to150" + path_NoPU , CP, cmssw, geometry, 0, 242578, 0, 1);
-  Datasets SingleTauPU140_PhaseIISpring17D("SingleTau-PU140", "SingleTau_PU140", "/SingleTau_FlatPt-8to150" + path_PU140, CP, cmssw, geometry, 0, 244605, 0, 1);
-  Datasets SingleTauPU200_PhaseIISpring17D("SingleTau-PU200", "SingleTau_PU200", "/SingleTau_FlatPt-8to150" + path_PU200, CP, cmssw, geometry, 0, 245355, 0, 1);
-  
-  Datasets TTNoPU_PhaseIISpring17D("TT-NoPU-pilot"  , "/TT_TuneCUETP8M1_14TeV-powheg-pythia8/" + path_NoPU , "", CP, cmssw, geometry, 0, 5000, 24, 2);
-  Datasets TTPU140_PhaseIISpring17D("TT-PU140-pilot", "/TT_TuneCUETP8M1_14TeV-powheg-pythia8/" + path_PU140, "", CP, cmssw, geometry, 0, 5000, 24, 2);
-  Datasets TTPU200_PhaseIISpring17D("TT-PU200-pilot", "/TT_TuneCUETP8M1_14TeV-powheg-pythia8/" + path_PU200, "", CP, cmssw, geometry, 0, 5000, 24, 2);
+  Datasets SingleTauNoPU_PhaseIISpring17D("SingleTau_NoPU"  , "SingleTau_NoPU" , "/SingleTau_FlatPt-8to150" + path_NoPU , CP, cmssw, geometry, 0, 242578, 0, 1);
+  Datasets SingleTauPU140_PhaseIISpring17D("SingleTau_PU140", "SingleTau_PU140", "/SingleTau_FlatPt-8to150" + path_PU140, CP, cmssw, geometry, 0, 244605, 0, 1);
+  Datasets SingleTauPU200_PhaseIISpring17D("SingleTau_PU200", "SingleTau_PU200", "/SingleTau_FlatPt-8to150" + path_PU200, CP, cmssw, geometry, 0, 245355, 0, 1);
   
+  Datasets TTNoPU_PhaseIISpring17D("TT_TuneCUETP8M1_14TeV_NoPU"  , "/TT_TuneCUETP8M1_14TeV-powheg-pythia8/" + path_NoPU , "", CP, cmssw, geometry, 0, 5000, 24, 2);
+  Datasets TTPU140_PhaseIISpring17D("TT_TuneCUETP8M1_14TeV_PU140", "/TT_TuneCUETP8M1_14TeV-powheg-pythia8/" + path_PU140, "", CP, cmssw, geometry, 0, 5000, 24, 2);
+  Datasets TTPU200_PhaseIISpring17D("TT_TuneCUETP8M1_14TeV_PU200", "/TT_TuneCUETP8M1_14TeV-powheg-pythia8/" + path_PU200, "", CP, cmssw, geometry, 0, 5000, 24, 2);
+    
   datasets_PhaseIISpring17D.push_back(SingleNeutrinoPU140_PhaseIISpring17D);
   datasets_PhaseIISpring17D.push_back(SingleNeutrinoPU200_PhaseIISpring17D);
   datasets_PhaseIISpring17D.push_back(SinglePion0NoPU_PhaseIISpring17D);
diff --git a/NtupleAnalysis/src/Framework/src/TreeAnalyserBase.C b/NtupleAnalysis/src/Framework/src/TreeAnalyserBase.C
index a2e0f7eaba0..ac5b84f0278 100644
--- a/NtupleAnalysis/src/Framework/src/TreeAnalyserBase.C
+++ b/NtupleAnalysis/src/Framework/src/TreeAnalyserBase.C
@@ -28,10 +28,9 @@ void TreeAnalyserBase::Initialize(const std::string MyName_,
   else prefix = MyName + "_";
 
   if (Text_ == "") postfix = "";
-  else prefix = "_" + Text_;
+  else postfix = "_" + Text_;
 
-  // if (Text == "") outFileName = MyName + "_Histograms_" + SampleName + ".root";
-  // else outFileName = MyName + "_Histograms_" + SampleName + "_" + Text + ".root";
+  // Create the output file name
   outFileName = prefix + "histograms-" + SampleName_ + postfix + ".root";
   outFile     = new TFile(outFileName.c_str(),"Recreate");
 
diff --git a/NtupleAnalysis/src/Tracking/Tracking.C b/NtupleAnalysis/src/Tracking/Tracking.C
index 2974030ba7f..e7c22895538 100644
--- a/NtupleAnalysis/src/Tracking/Tracking.C
+++ b/NtupleAnalysis/src/Tracking/Tracking.C
@@ -132,7 +132,7 @@ void Tracking::Loop()
   for (int jentry = 0; jentry < nEntries; jentry++, nEvts++)
     {
 
-      if(1) cout << "\tEntry = " << jentry << endl;
+      if(DEBUG) cout << "\tEntry = " << jentry << endl;
 
       // Load the tree && Get the entry
       Long64_t ientry = LoadTree(jentry);
@@ -140,67 +140,85 @@ void Tracking::Loop()
       nb = fChain->GetEntry(jentry);
       nbytes += nb;
 
+
+      // =============================================================================
       // GenParticles Collection
+      // =============================================================================
+      
       if (DEBUG) cout << "=== GenParticles (" << GenP_Pt->size() << ")" << endl;
       vector<GenParticle> GenParticles = GetGenParticles(false);
       if (DEBUG) PrintGenParticleCollection(GenParticles);
 
+
+      
+      // =============================================================================
       // Tracking Particle Collections
+      // =============================================================================
       if (DEBUG) cout << "=== Tracking Particles (" << TP_Pt->size() << ")" << endl;
       vector<TrackingParticle> TPs  = GetTrackingParticles(false);
       sort( TPs.begin(), TPs.end(), PtComparatorTP() ); // not sorted by default
       if (DEBUG) PrintTrackingParticleCollection(TPs);
       
+
+
+      // =============================================================================
       // TTTracks Collections
+      // =============================================================================
       if (DEBUG) cout << "=== TTracks (" << L1Tks_Pt->size() << ")" << endl;
       vector<TTTrack> TTTracks = GetTTTracks(tk_minPt, tk_minEta, tk_maxEta, tk_maxChiSqRed, tk_minStubs, tk_minStubsPS, tk_maxStubsPS, tk_nFitParams, false);
       sort( TTTracks.begin(), TTTracks.end(), PtComparatorTTTrack() ); // not sorted by default
       if (DEBUG) PrintTTTrackCollection(TTTracks);
 
-      // Tau Collection
-      if (1) cout << "=== Taus (" << tauEt->size() << ")" << endl;
-      //vector<L1JetParticle> L1Taus = GetL1Taus(false);
-      // FIXME: From L1Taus, we should construct L1TkTaus_Calo, see CaloTk.C for details
-      // if (DEBUG) PrintGenParticleCollection(GenParticles); //FIXME
-
-      // Fill TTTracks resolution histograms (only for cases where there is a matching TP)
+      // For-loop: TTTracks
       for (vector<TTTrack>::iterator tk = TTTracks.begin(); tk != TTTracks.end(); tk++)
-	  {
-	    double tk_pt     = tk->getPt();
-	    double tk_eta    = tk->getEta();
-	    double tk_phi    = tk->getPhi();
-	    
-	    double tp_pt     = 0;
-	    double tp_eta    = 0;
-	    double tp_phi    = 0;	    
-	    
-	    int tp_index = 0;
-	    tp_index = tk->getTPIndex();
-
-        // If there is a TP matching to TTTrack, fill histograms
-	    if(tp_index >= 0){
-
-          TrackingParticle tp = GetTrackingParticle(tp_index);
-
-          tp_pt          = tp.getPt();
-          tp_eta         = tp.getEta();
-          tp_phi         = tp.getPhi();
-
-    	  hL1Tks_Pt->Fill(tk_pt);
+	{
+	  double tk_pt     = tk->getPt();
+	  double tk_eta    = tk->getEta();
+	  double tk_phi    = tk->getPhi();
+	  
+	  double tp_pt     = 0;
+	  double tp_eta    = 0;
+	  double tp_phi    = 0;	    
+	  
+	  int tp_index = 0;
+	  tp_index = tk->getTPIndex();
+
+	  // If there is a TP matching to TTTrack
+	  if(tp_index >= 0)
+	    {
+	      
+	      TrackingParticle tp = GetTrackingParticle(tp_index);
+	      
+	      tp_pt  = tp.getPt();
+	      tp_eta = tp.getEta();
+	      tp_phi = tp.getPhi();
+
+	      // Fill resolution histograms (only for cases where there is a matching TP)	      
+	      hL1Tks_Pt->Fill(tk_pt);
 	      hL1Tks_Eta->Fill(tk_eta);
 	      hL1Tks_Phi->Fill(tk_phi);
-
-          hL1Tks_Pt_Res->Fill( (tk_pt-tp_pt)/tp_pt );
-          hL1Tks_Eta_Res->Fill( (tk_eta-tp_eta)/tp_eta );
-          hL1Tks_Phi_Res->Fill( (tk_phi-tp_phi)/tp_phi );
-
+	      
+	      hL1Tks_Pt_Res->Fill( (tk_pt-tp_pt)/tp_pt );
+	      hL1Tks_Eta_Res->Fill( (tk_eta-tp_eta)/tp_eta );
+	      hL1Tks_Phi_Res->Fill( (tk_phi-tp_phi)/tp_phi );
+	      
 	      hTP_Pt->Fill(tp_pt);
 	      hTP_Eta->Fill(tp_eta);
 	      hTP_Phi->Fill(tp_phi);
-          
-	    }
-	    
-	  }
+	      
+	    }//tp_index >= 0
+	  
+	}//for-loop: TTTracks
+      
+
+      // =============================================================================
+      // L1Taus Collection
+      // =============================================================================
+      if (DEBUG) cout << "=== Taus (" << tauEt->size() << ")" << endl;
+      //vector<L1JetParticle> L1Taus = GetL1Taus(false);
+      // FIXME: From L1Taus, we should construct L1TkTaus_Calo, see CaloTk.C for details
+      // if (DEBUG) PrintGenParticleCollection(GenParticles); //FIXME
+
 
     // Fill L1TkTaus_Calo resoltion histograms 
 /*    for (vector<L1TkTauParticle>::iterator tau = L1TkTaus_Calo.begin(); tau != L1TkTaus_Calo.end(); tau++) //FIXME: construct L1TkTaus_Calo
@@ -260,8 +278,8 @@ void Tracking::BookHistos_(void)
   histoTools_.BookHisto_1D(hL1Tks_Eta, "L1Tks_MatchedToTP_Eta", "; #eta; Entries", 130,  -2.6,  +2.6);
   histoTools_.BookHisto_1D(hL1Tks_Phi, "L1Tks_MatchedToTP_Phi", "; #phi (rads); Entries", 128,  -3.2,  +3.2);
   histoTools_.BookHisto_1D(hL1Tks_Pt_Res, "L1Tks_MatchedToTP_Pt_Res", "; (p_{T}^{track}-p_{T}^{particle})/p_{T}^{particle}; Entries", 200,  -1.0,  +1.0);
-  histoTools_.BookHisto_1D(hL1Tks_Eta_Res, "L1Tks_MatchedToTP_Eta_Res", "; (#eta^{track}-#eta^{particle})/#eta^{particle}; Entries", 160,  -0.2,  +0.2);
-  histoTools_.BookHisto_1D(hL1Tks_Phi_Res, "L1Tks_MatchedToTP_Phi_Res", "; (#phi^{track}-#phi^{particle})/#phi}^{particle} ; Entries", 160,  -0.2,  +0.2);
+  histoTools_.BookHisto_1D(hL1Tks_Eta_Res, "L1Tks_MatchedToTP_Eta_Res", "; (#eta^{track}-#eta^{particle})/#eta^{particle}; Entries", 400,  -0.2,  +0.2);
+  histoTools_.BookHisto_1D(hL1Tks_Phi_Res, "L1Tks_MatchedToTP_Phi_Res", "; (#phi^{track}-#phi^{particle})/#phi}^{particle} ; Entries", 400,  -0.2,  +0.2);
   histoTools_.BookHisto_1D(hTP_Pt, "TP_MatchedToL1Tk_Pt", "; p_{T} (GeV/c); Entries", 200,  +0.0,  +200.0);
   histoTools_.BookHisto_1D(hTP_Eta, "TP_MatchedToL1Tk_Eta", "; #eta; Entries", 130,  -2.6,  +2.6);
   histoTools_.BookHisto_1D(hTP_Phi, "TP_MatchedToL1Tk_Phi", "; #phi (rads); Entries", 128,  -3.2,  +3.2);  
diff --git a/NtupleAnalysis/src/Tracking/Tracking.h b/NtupleAnalysis/src/Tracking/Tracking.h
index f1b67883373..787433590a2 100644
--- a/NtupleAnalysis/src/Tracking/Tracking.h
+++ b/NtupleAnalysis/src/Tracking/Tracking.h
@@ -38,7 +38,7 @@ class Tracking : public TreeAnalyserMC{
 	  const int maxEvents_ = -1, 
 	  TTree* tree=0) : 
   
-  TreeAnalyserMC("TTTracks", SamplePath, SampleName, text_, maxEvents_, tree) 
+  TreeAnalyserMC("", SamplePath, SampleName, text_, maxEvents_, tree) 
     { 
       auxTools_.StopwatchStart();
       mcSample = SampleName;
diff --git a/NtupleAnalysis/src/Tracking/work/plotMC.py b/NtupleAnalysis/src/Tracking/work/plotMC.py
index b6474e53ca4..e25be851397 100755
--- a/NtupleAnalysis/src/Tracking/work/plotMC.py
+++ b/NtupleAnalysis/src/Tracking/work/plotMC.py
@@ -10,7 +10,7 @@ Usage (multiple plots):
 
 Last Used:
 ./plotMC.py -m multicrab_CaloPlusTracks_v61XSLHC6_20170420T1537/ json/L1TkTau/Multiplicity.json -i "VBF|Neutrino"
-./plotMC.py -m multicrab_CaloPlusTracks_v61XSLHC6_20170420T1537/ json/*/*.json -i "VBF|Neutrino"
+./plotMC.py -m multicrab_CaloPlusTracks_v61XSLHC6_20170420T1537/ json/*/*.json -i "SingleTau_P140|SingleTau_P200|"
 '''
 
 #================================================================================================
diff --git a/NtupleAnalysis/src/Tracking/work/run.cc b/NtupleAnalysis/src/Tracking/work/run.cc
index 314ea50fcce..2ef4c31f94c 100755
--- a/NtupleAnalysis/src/Tracking/work/run.cc
+++ b/NtupleAnalysis/src/Tracking/work/run.cc
@@ -4,7 +4,7 @@
 //
 // Example:
 // root -l
-// root[0] .x run.cc("multicrab_CaloTk_v910p2_test", "SingleTau-PU140", "", -1);
+// root[0] .x run.cc("multicrab_CaloTk_v910p2_20170520T1958", "SingleTau-PU140", "", -1);
 //
 //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
 #include "../Tracking.C+" // how to compile macro in ROOT6 (compatible with ROOT5 as well)
@@ -15,11 +15,12 @@ void run(const std::string MulticrabDir = "",
 	 const int maxEvents = -1)
 {
 
-// Alexandro's files
-  // const std::string absolutePath = "/Users/attikis/hltaus/rootFiles/TTrees/P2L1T_HLTaus_91X";
-     const std::string absolutePath = "/afs/cern.ch/user/a/attikis/workspace/multicrab/";
+  // Alexandros' files
+  const std::string absolutePath = "/Users/attikis/hltaus/rootFiles/TTrees/P2L1T_HLTaus_91X";
+  // const std::string absolutePath = "/afs/cern.ch/user/a/attikis/workspace/multicrab/";
   // const std::string absolutePath = "/Users/attikis/disk/hltaus/rootFiles/TTrees/P2L1T_HLTaus_91X/";
-// Santeri's files
+  
+  // Santeri's files
   // const std::string absolutePath = "/afs/cern.ch/work/s/slaurila/public/HLTaus/CMSSW_9_1_0_pre2/src/HLTausAnalysis/NtupleAnalysis/src/Tracking/work";
   
   Tracking macro(absolutePath + "/" + MulticrabDir, SampleName, text, maxEvents);

